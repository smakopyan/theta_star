import rclpy
import numpy as np
from tensorflow import keras
from env import TurtleBotEnv
from actor import ImprovedActor

# –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
actor = keras.models.load_model('ppo_turtlebot_actor.keras')
rclpy.init() 
# –°–æ–∑–¥–∞—ë–º —Å—Ä–µ–¥—É
env = TurtleBotEnv()

num_episodes = 10  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤
total_rewards = []
total_steps = []
success_count = 0

for episode in range(num_episodes):
    state = env.reset()
    state = np.reshape(state, [1, env.observation_space.shape[0]])
    
    done = False
    episode_reward = 0
    steps = 0

    while not done:
        action_probs = actor(state)
        action = np.argmax(action_probs)  # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é

        next_state, reward, done, info = env.step(action)
        next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])

        state = next_state
        episode_reward += reward
        steps += 1

        if "goal_reached" in info and info["goal_reached"]:
            success_count += 1

    total_rewards.append(episode_reward)
    total_steps.append(steps)

    print(f'Episode {episode + 1}: Reward = {episode_reward}, Steps = {steps}')

env.close()

# üîπ –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
print("\n=== Evaluation Results ===")
print(f'‚úÖ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {np.mean(total_rewards):.2f}')
print(f'‚úÖ –°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ —à–∞–≥–æ–≤: {np.mean(total_steps):.2f}')
print(f'‚úÖ –£—Å–ø–µ—à–Ω—ã–µ —ç–ø–∏–∑–æ–¥—ã: {success_count} / {num_episodes} ({(success_count / num_episodes) * 100:.1f}%)')